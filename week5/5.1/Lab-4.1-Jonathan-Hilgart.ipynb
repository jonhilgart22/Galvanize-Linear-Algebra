{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSCI 6001 5.1.Lab\n",
    "QR Factorization\n",
    "So far you have already learned the $LU$ decomposition/factorization, which was by far the most common method of obtaining a solution to a linear system for some time, until the Francis method of $QR$ decomposition came about in the 60's. The Francis method $QR$ decomposition not only gives the eigenvectors to a matrix, but the eigenvalues and the solution to the system as well. $LU$ is still the most common method for decomposing small matrices as it's somewhat faster, but less stable.\n",
    "Shortly after this, methods involving Givens rotations and Householder reflections came about (better stability). The latter method is the best for larger matrices. For very large matrices only approximate methods are possible even today due to the high order ( $O(N^{3})$ ) of all of these algorithms.\n",
    "Concepts\n",
    "It is a theorem that any nonsingular (invertible) matrix can be factored into a product of two matrices: A matrix $Q$ of orthogonal vectors (representing an image-preserving map), and an upper-right triangular matrix $R$ much like the $U$ matrix of the $LU$ factorization. From this we may obtain a unique solution to the system.\n",
    "Factoring noninvertible matrices (?!?!)\n",
    "So far all your effort (and for the rest of this class) has been bent on factoring invertible or nonsingular matrices. It may seem perhaps that there are at least as many examples of noninvertible or singular matrices that might need to be factored.\n",
    "In this case, matrix factorization proceeds by a least-squares algorithm. This will be covered somewhat later. Least-squares methods are what are used in industry to provide matrix factorizations at scale.\n",
    "Construction of the QR factorization\n",
    "Let us describe a square matrix, A:\n",
    "$$ {\\bf{A}} = \\{\\vec{a_{,1}}, \\vec{a_{,2}}, \\vec{a_{,3}}, \\cdots, \\vec{a_{,n}}\\}$$\n",
    "Now let us apply the Gram-Schmidt process to the columns of ${\\bf{A}}$, to obtain a new set of orthonormal column vectors. These vectors describe an orthornormal projection of the image space of the original ${\\bf{A}}$:\n",
    "$$q_1 = \\dfrac{u_{,1}}{\\|u_{,1}\\|}; u_{,1} = a_{,1}$$$$q_2 =  \\dfrac{u_{,2}}{\\|u_{,2}\\|}; u_{,2} = a_{,2}-(a_{,2} \\cdot u_{,1}) u_{,1}$$$$q_3 = \\dfrac{u_{,3}}{\\|u_{,3}\\|}; u_{,3} = a_{,3}-(a_{,3} \\cdot u_{,2}) u_{,2} -(a_{,3} \\cdot u_{,1}) u_{,1}$$$$\\vdots$$$$q_{k+1} = \\dfrac{u_{,k+1}}{\\|u_{,k+1}\\|}; u_{,k+1} = a_{,k+1} - \\sum_{n=1}^{k}(a_{,k+1} \\cdot u_{,n})u_{,n}$$\n",
    "And hence form the $N$ columns of the matrix ${\\bf{Q}}$:\n",
    "$${\\bf{Q}} = \\begin{bmatrix} \\vec{q_1} &; \\vec{q_2} &; \\cdots &; \\vec{q_{N}}\\end{bmatrix}$$\n",
    "Now it so turns out that this happens to be a decomposition of ${\\bf{A}}$ if we realize that we can dot the rows of ${\\bf{Q}}$ with another matrix if we want to simply return the elements of ${\\bf{A}}$:\n",
    "$$\\begin{bmatrix} a_{1,1} &; a_{1,2} &; \\cdots &amp; a_{1,N}\\\\ a_{2,1} &amp; a_{2,2} &amp; \\cdots &amp; a_{2,N}\\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ a_{N,1} &amp; a_{M,2} &amp; \\cdots &amp; a_{N,N}\n",
    "\\end{bmatrix} = \\begin{bmatrix} q_{1,1} &amp; q_{1,2} &amp; \\cdots &amp; q_{1,N}\\\\ q_{2,1} &amp; q_{2,2} &amp; \\cdots &amp; q_{2,N}\\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ q_{M,1} &amp; q_{M,2} &amp; \\cdots &amp; q_{M,N}\n",
    "\\end{bmatrix}\\begin{bmatrix} q_{1,1}a_{1,1} &amp; q_{1,2}a_{1,2} &amp; \\cdots &amp; q_{1,N}a_{1,N}\\\\ 0 &amp; q_{2,2}a_{2,2} &amp; \\cdots &amp; q_{2,N}a_{2,N}\\\\ 0 &amp; 0 &amp; \\cdots &amp; \\cdots \\\\ 0 &amp; 0 &amp; 0 &amp; q_{N,N}a_{N,N}\n",
    "\\end{bmatrix} $$\n",
    "Thus:\n",
    "$${\\bf{Q}} = \\begin{bmatrix} | &amp; | &amp; |\\\\\n",
    "                             q_{,1} &amp; q_{,2} &amp; q_{,3} \\\\  \n",
    "                             | &amp; | &amp; | \\end{bmatrix}$$$${\\bf{R}} = \\begin{bmatrix} a_{,1} \\cdot q_{,1} &amp; a_{,2} \\cdot q_{,1} &amp; a_{,3} \\cdot q_{,1} \\\\\n",
    "                             0 &amp; a_{,2} \\cdot q_{,2} &amp; a_{,3} \\cdot q_{,2}  \\\\  \n",
    "                             0 &amp; 0 &amp; a_{,3} \\cdot q_{,3} \\end{bmatrix}$$\n",
    "This is because each element of ${\\bf{Q}}$ contains the magnitude of the dot product, thus the inner product of each row by column results in the appropriate element of ${\\bf{A}}$.\n",
    "Example:\n",
    "Compute the QR factorization for the matrix using Gramm-Schmidt:\n",
    "$$\\begin{bmatrix} 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\end{bmatrix}$$$$ \\vec{u^{(1)}} = \\vec{a^{(1)}}$$$$\\vec{q_{,1}} =\\dfrac{u_{,1}}{\\|u_{,1}\\|} = \\dfrac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\1\\\\0\\end{bmatrix}$$$$ \\vec{u_{,2}} = a_{,2}-(a_{,2} \\cdot u_{,1}) u_{,1} = \\begin{bmatrix}1\\\\0\\\\1\\end{bmatrix} - \\dfrac{1}{\\sqrt{2}}\\begin{bmatrix}\\dfrac{1}{\\sqrt{2}}\\\\\\dfrac{1}{\\sqrt{2}}\\\\0\\end{bmatrix} =  \\begin{bmatrix}\\dfrac{1}{2}\\\\-\\dfrac{1}{2}\\\\1\\end{bmatrix}$$$$\\vec{q_{,2}} = \\dfrac{u_{,2}}{\\|u_{,2}\\|} = \\begin{bmatrix}\\dfrac{1}{\\sqrt{6}}\\\\-\\dfrac{1}{\\sqrt{6}}\\\\ \\dfrac{2}{\\sqrt{6}}\\end{bmatrix}$$$$ \\vec{u_{,3}} = a_{,3}-(a_{,3} \\cdot u_{,2}) u_{,2}-(a_{,3} \\cdot u_{,1}) u_{,1} $$$$ \\vec{u_{,3}} = \\begin{bmatrix}0\\\\1\\\\1\\end{bmatrix} -\\dfrac{1}{\\sqrt{6}}\\begin{bmatrix}\\dfrac{1}{\\sqrt{6}}\\\\-\\dfrac{1}{\\sqrt{6}}\\\\ \\dfrac{2}{\\sqrt{6}}\\end{bmatrix} - \\dfrac{1}{\\sqrt{2}}\\begin{bmatrix}\\dfrac{1}{\\sqrt{2}}\\\\\\dfrac{1}{\\sqrt{2}}\\\\0\\end{bmatrix} = \\begin{bmatrix}-\\dfrac{1}{\\sqrt{3}}\\\\\\dfrac{1}{\\sqrt{3}}\\\\ \\dfrac{1}{\\sqrt{3}}\\end{bmatrix}$$\n",
    "Thus we have a full basis for the image space of $\\bf{Q}$:\n",
    "$$\\left\\{ \\begin{bmatrix}\\dfrac{1}{\\sqrt{2}} \\\\ \\dfrac{1}{\\sqrt{2}}\\\\0 \\end{bmatrix}, \\begin{bmatrix}\\dfrac{1}{\\sqrt{6}}\\\\-\\dfrac{1}{\\sqrt{6}}\\\\ \\dfrac{2}{\\sqrt{6}}\\end{bmatrix}, \\begin{bmatrix}-\\dfrac{1}{\\sqrt{3}}\\\\\\dfrac{1}{\\sqrt{3}}\\\\ \\dfrac{1}{\\sqrt{3}}\\end{bmatrix}  \\right\\}$$\n",
    "Now we can construct Q confidently:\n",
    "$${\\bf{Q}} = \\begin{bmatrix}\\dfrac{1}{\\sqrt{2}} &amp; \\dfrac{1}{\\sqrt{6}} &amp; -\\dfrac{1}{\\sqrt{3}} \\\\ \\dfrac{1}{\\sqrt{2}} &amp; -\\dfrac{1}{\\sqrt{6}} &amp; \\dfrac{1}{\\sqrt{3}} \\\\ 0 &amp; \\dfrac{2}{\\sqrt{6}} &amp; \\dfrac{1}{\\sqrt{3}} \\end{bmatrix}$$\n",
    "And R follows simply with elements that are the proscribed dot products:\n",
    "$${\\bf{R}} = \\begin{bmatrix}\\dfrac{\\sqrt{2}}{2} &amp; \\dfrac{1}{\\sqrt{3}} &amp; \\dfrac{1}{\\sqrt{2}} \\\\ 0 &amp; \\dfrac{3}{\\sqrt{6}} &amp; \\dfrac{1}{\\sqrt{6}} \\\\ 0 &amp; 0 &amp; \\dfrac{2}{\\sqrt{3}} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1:\n",
    "Use the Gramm-Schmidt algorithm (by hand) to produce the QR factorization of the following matrix:\n",
    "$$ A = \\begin{bmatrix} -2 &; 0 &; 1\\\\ 1 &; -2 &; 1 \\\\ 1 &; -1 &; 0\\end{bmatrix}$$\n",
    "Show most of your steps. You may use the computer to check your steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vec1 = np.array([-2,1,1])\n",
    "\n",
    "\n",
    "vec2=np.array([0,-2,-1])\n",
    "\n",
    "vec3 = np.array([1,1,0])\n",
    "\n",
    "\n",
    "A = np.array([[-2,0,1],\n",
    "             [1,-2,1],\n",
    "             [1,-1,0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for the projection (A onto B)\n",
    "\n",
    "$(B * A) / norm(B) ^2  *B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proj(c2,c1): return (c1.dot(c2)/(c1.dot(c1)))*c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.81649658,  0.40824829,  0.40824829])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "\n",
    "vec1_n = vec1/sqrt(4+1+1)\n",
    "vec1_n \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  -1.5 -0.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.53452248, -0.80178373, -0.26726124])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2_p = vec2 - proj(vec2,vec1_n)\n",
    "print(vec2_p)\n",
    "vec2_n  = vec2_p / np.linalg.norm(vec2_p)\n",
    "vec2_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.21821789,  0.43643578, -0.87287156])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec3_p = vec3 - proj(vec3,vec1_n) - proj(vec3,vec2_n)\n",
    "vec3_n = vec3_p/np.linalg.norm(vec3_p)\n",
    "vec3_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.81649658, -0.53452248, -0.21821789],\n",
       "       [ 0.40824829, -0.80178373,  0.43643578],\n",
       "       [ 0.40824829, -0.26726124, -0.87287156]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = np.array([vec1_n,vec2_n,vec3_n]).T\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 2:\n",
    "Write the Gram-Schmidt algorithm using the code stub below. You may also write your own GS algorithm if you want to pursue more optimized strategies. There are a number of ways that the GS can be written using broadcasting or matrix-based strategies. Tomorrow, we will write the modified GS algorithm that actually ends up being much simpler to implement than the below strategy.\n",
    "Here we actually store the current state of the GS basis in a matrix $Q$ just as if we were writing it by hand. As we build up the GS basis, we iterate through each of the vectors, calculating the projection of the $Q$ vectors $v$ onto each of the input vectors $u$. Then we subtract these projections from the current vector taken from the original set. After normalization, we have our new orthonormal vector that we can add into Q. The algorithm continues on until we have orthonormalized every vector in $A$.\n",
    "In [182]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gram_schmidt(A, col_vecs=True, norm=True):\n",
    "    size = len(A)\n",
    "    u = np.zeros(shape = (size,size))\n",
    "    shape = np.shape(A)\n",
    "    Q = []\n",
    "    e=[]\n",
    "    A_T = A.T\n",
    "    # assumes column vector format of X\n",
    "    #print(u,'intial u')\n",
    "    # you may want to check the determinant (why?)\n",
    "    if shape[0]==shape[1] and np.linalg.det(A)!=0:\n",
    "        #Square matrix and can decompose\n",
    "\n",
    "        ## now cycle through the columns\n",
    "        for j, col in enumerate(A_T):\n",
    "            u[j]=A_T[j] # U will store our projections\n",
    "            \n",
    "            if j == 0:\n",
    "                #e to store our normalized vectors\n",
    "                e.append(A_T[j]/np.linalg.norm(A_T[j]))\n",
    "                Q.append(e[j]) ## The first vector is normalized \n",
    "                #print(Q,'Q')\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                #u should store the projections\n",
    "                \n",
    "                u[j] = A_T[j]-sum([A_T[j].dot(e[i])*(e[i])  for i in range(len(e))])\n",
    "\n",
    "                #print(u[j],'u[j]')\n",
    "                \n",
    "                e.append(u[j]/np.linalg.norm(u[j]))\n",
    "                #print(e,'e')\n",
    "                \n",
    "                Q.append(e[j])\n",
    "                \n",
    "               \n",
    "    # Initialize an empty matrix to store the current state of the GS basis\n",
    "\n",
    "    # * for each vector u in At (the column matrix) (for j, col in enumerate At - go through columns one by one)\n",
    "        \n",
    "        # * for each of the other vectors v in Q\n",
    "            # you may need to cast v into the extra dimensions cast into numpy here\n",
    "            \n",
    "            # * remove proj_v(u) from u\n",
    "            # => u = u-(v.u/v.v)*v\n",
    "        # * normalize u - np.linalg.norm\n",
    "    \n",
    "        # you'll need to add the new u to your Q matrix\n",
    "        \n",
    "        #Q.append(u.flatten())\n",
    "    return(np.array(Q).T)\n",
    "    # * return the Q matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.81649658 -0.53452248 -0.21821789]\n",
      " [ 0.40824829 -0.80178373  0.43643578]\n",
      " [ 0.40824829 -0.26726124 -0.87287156]] Q\n",
      "[[ -2.00000000e+00   7.44926802e-16   1.00000000e+00]\n",
      " [  1.00000000e+00  -2.00000000e+00   1.00000000e+00]\n",
      " [  1.00000000e+00  -1.00000000e+00  -4.38137812e-16]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[-2, 0, 1 ],[1, -2, 1],[1, -1, 0]])\n",
    "Q = gram_schmidt(A)\n",
    "R = Q.dot(A)\n",
    "\n",
    "R = Q.T.dot(A)\n",
    "print(Q,'Q')\n",
    "print(Q.dot(R))\n",
    "\n",
    "\n",
    "A = np.array([[-2, 0, 1 ],[1, -2, 1],[1, -1, 0]])\n",
    "\n",
    "assert np.allclose(Q.dot(R),A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = np.array([[2,3,4],\n",
    "             [5,6,7],\n",
    "             [2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(T[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.array([[4,3,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A[[0]].dot(T[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
